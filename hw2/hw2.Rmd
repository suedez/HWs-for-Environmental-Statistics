---
title: "hw2"
output: html_document
---

##1
###1a 
```{r}
set.seed(567)
epsilon <- rnorm(100000,0)
corre_coeff <- seq(-0.9, 0.9, by = 0.1)
tsMX <- matrix(nrow = 100000, ncol = length(corre_coeff))
for (j in (1:length(corre_coeff))){
  ts <- rep(0,100000) # initialize timeseries
  for (i in (2:100000)){
    ts[i] <- epsilon[i] + corre_coeff[j]*ts[i-1]
  }
  tsMX[,j] <- ts
}

```


###1b

```{r}
split_factor <- as.vector(mapply(rep, 1:1000, times = rep(100,1000)))
split_factor <- as.factor(split_factor)
var_mu <- function(x){
  MEANs <- tapply(x,split_factor,mean)
  return(var(MEANs))
}

VARmu <- apply(tsMX, MARGIN = 2, var_mu)
cat('The variance of means of different of each set of time series with autocorrelation coefficient -0.9, -0.8 ...0, 0.1, ... 0.9 are', VARmu)
```

###1c
    
    Calculate the effective sample size by
$$
N_{eff}=\frac{\sigma^2} {var(\bar x)}
$$

```{r}
Var <- apply(tsMX, MARGIN = 2, var)
size_me <- round(Var / VARmu) + 1
cat('Effective sample size for each data set are ', size_me)
```

###1d

```{r}
size_coeff <- rep(0,19)
for (i in (1:19)){
  size_coeff[i] <- (1-corre_coeff[i])/(1+corre_coeff[i])
}
size_wilk <- 100 * size_coeff
plot(corre_coeff, size_me,type = 'l',col = 'red', ylim = c(0, 2000),
     xlab = 'autocorrelation coefficient', ylab = 'effective sample size',
     main = 'Effective Sample Size')
lines(corre_coeff, size_wilk, col = 'blue')
legend('topright', title = 'Methods', c('empirical','Wilks'), 
       lty = c(1,1), col = c('red','blue'))

```

    The eccective sample sizes result from different methods match well.

##2
###2a&b

```{r}
dataset1 <- tsMX[(1:100), 16] # cc=0.6
x <- (1:100)
y <- dataset1
var1 <- (summary(lm(y~x))$coefficients[2,2])^2
estimator1 <- summary(lm(y~x))$coefficients[2,1]
OLSr <- ar(dataset1,method = 'ols',order.max = 1)
print(paste('estimator is', estimator1, 
            ' variance in estimator is',
            var1))
```
    
    The trend of the data can be expressed as 
$$
Y=-0.68851 + 0.00836*x
$$
    Where x=1,2,3...100
    
###2c

```{r}
## calculate block size
phil <- OLSr$ar
effectiveN <- 100 * (1 - phil) / (1 + phil)
L <- 1
err <- 1
tol <- 1e-6
while(err > tol){
  f <- L - (100  - L + 1) ^ (2/3 * (1-effectiveN/100))
  J <- 1 + (2/3 * (1-effectiveN/100)) * 
    ((100 - L + 1) ^ (2/3 * (1-effectiveN/100) - 1))
  err = abs(f)
  dl = as.numeric(solve(J) %*% f)
  L = L - dl
}
print(paste('the block size is', round(L) + 1))

acf(dataset1)

## bootstrap
block_pool <- matrix(nrow = 11, ncol = 90)
for (i in (1:90)){
  block_pool[,i] <- dataset1[i:(i+10)]
}
indexpool <- (1:90)

philBS <- c()
x <- 1:99
for (i in (1:1000)){
 index <- sample(indexpool,9) # 11*9=99 close to 100
 dataseti <- c()
 for (j in (1:9)){
   dataseti <- c(dataseti, block_pool[,index[j]])
 }
 OLSi <- summary(lm(dataseti~x))$coefficients[2,1]
 philBS[i] <- OLSi
}
print(paste("the variance in estimator from bootstrap method is ", var(philBS)))
      
print(paste("the expected estimator from bootstrap method is ", mean(philBS)))
```
    As it shownd in the ACF plot, the acf is significantly close to 0 at lag 11, which means this block size is able to contain almost all correlation in one block.

##2d

```{r message=TRUE, warning=FALSE}
cc <- 0.6
x <- 1:100
philG <- c() #Generate
set.seed(567)
for (i in (1:1000)){
  epsiloni <- rnorm(100,0)
  dataseti <- rep(0,100)
  for (j in (2:100)){
    dataseti[j] <- epsiloni[j] + cc * dataseti[j-1]
  }
  OLSi <- summary(lm(dataseti~x))$coefficients[2,1]
  philG[i] <- OLSi
}

dfvar <- data.frame(BootStrap = c(var(philBS), mean(philBS)), 
                    Regenerate = c(var(philG), mean(philG)), OLS = c(var1, estimator1))
rownames(dfvar) <- c('variance', 'estimator')
print(paste("the variance in estimator from regenerating method is ", var(philG)))
print(paste("the expected estimator from regenerating method is ", mean(philG)))

plot(density(philBS), col = 'red',
     main = 'estimator distribution', ylim = c(0,100))
lines(density(philG), col = 'blue')
legend('topleft', c('Bootstrap','Regenerate data'), 
             lty = c(1,1), col = c('red','blue'),cex = 0.6)

dfvar
```
    
    The variance resulted from bootstrap method is more close to 1/(100000/11), maybe it is because the bootstrap method take the autocorrelation into account? Actually I'm not really clear with this part. 
    
##3

###3a

    I calculated seasonal average HDD and CDD for each year, so I got two 75-length time series.

```{r}
data_max<-read.csv('USW00023174_TMAX_clean.csv',header = T)
value_max<-data_max$value

data_min <- read.csv('USW00023174_TMIN_clean.csv',header = T)
value_min <- data_min$value
Nyear<-floor(length(value_max)/365)-1
Nyear_full <- floor(length(value_max)/365)-1
DAYofYEAR <-c(1:365)
date_1<-as.Date('1944-8-1')
date_2<-as.Date('1944-12-31')
interval1 <- as.integer(difftime(date_2,date_1,units='days'))
DAY1944 <- c((365-interval1):365)
day_ordered<-c(DAY1944,rep(DAYofYEAR,Nyear_full)) 
day_left <- length(value_max)-length(day_ordered) # day has the same order as the data
day_ordered <- c(day_ordered,c(1:day_left))

## separate a year into 12 month
daySEPseason <- day_ordered #day seperated by month
daySEPseason[(daySEPseason>=1 & daySEPseason<=59) | 
               (daySEPseason>=335 & daySEPseason<=365)] <-1 #DJF
daySEPseason[(daySEPseason>=60 & daySEPseason<=151)] <-2 #MAM
daySEPseason[daySEPseason>=152 & daySEPseason<=243] <-3 #JJA
daySEPseason[daySEPseason>=244 & daySEPseason<=334] <-4 #SON

seasonDATA_frame <- data.frame(daySEPseason,value_max)
seasonDATA_frameMIN <- data.frame(daySEPseason,value_min)
seasonDATA_frame <- seasonDATA_frame[seasonDATA_frame$value_max!=-999.9,]
seasonDATA_frameMIN <- seasonDATA_frameMIN[seasonDATA_frameMIN$value_min!=-999.9,]
factormax <- as.factor(seasonDATA_frame$daySEPseason)
factormin <- as.factor(seasonDATA_frameMIN$daySEPseason)
meanmax <- tapply(seasonDATA_frame$value_max, factormax, mean)
meanmin <- tapply(seasonDATA_frameMIN$value_min, factormin, mean)

seasonDATA_frame <- data.frame(daySEPseason,value_max)
seasonDATA_frameMIN <- data.frame(daySEPseason,value_min)

for (i in (1:4)){
  seasonDATA_frame$value_max[seasonDATA_frame$value_max == -999.9 & 
                               seasonDATA_frame$daySEPseason == i] <- meanmax[i]
  seasonDATA_frameMIN$value_min[seasonDATA_frameMIN$value_min == -999.9 & 
                                  seasonDATA_frameMIN$daySEPseason == i] <- meanmin[i]
  
}

DJF_max <- seasonDATA_frame$value_max[seasonDATA_frame$daySEPseason==1]
MAM_max <- seasonDATA_frame$value_max[seasonDATA_frame$daySEPseason==2]
JJA_max <- seasonDATA_frame$value_max[seasonDATA_frame$daySEPseason==3]
SON_max <- seasonDATA_frame$value_max[seasonDATA_frame$daySEPseason==4]

DJF_min <- seasonDATA_frameMIN$value_min[seasonDATA_frameMIN$daySEPseason==1]
MAM_min <- seasonDATA_frameMIN$value_min[seasonDATA_frameMIN$daySEPseason==2]
JJA_min <- seasonDATA_frameMIN$value_min[seasonDATA_frameMIN$daySEPseason==3]
SON_min <- seasonDATA_frameMIN$value_min[seasonDATA_frameMIN$daySEPseason==4]

## calculate HDD & CDD

HDD <- 15 - (DJF_max + DJF_min)/2 
HDD[HDD < 0] <- 0
CDD <- (JJA_max + JJA_min)/2 - 20 
CDD[CDD < 0] <- 0

CDDfactor <- as.vector(mapply(rep, 2:75, times = rep(92,74)))
CDDfactor <- c(rep(1, 31), CDDfactor) 
CDDfactor <- as.factor(CDDfactor)

HDDfactor <- as.vector(mapply(rep, 2:74, times = rep(90,73)))
HDDfactor <- c(rep(1, 31), HDDfactor, rep(75, 59)) 
HDDfactor <- as.factor(HDDfactor)

HDDmean <- tapply(HDD, HDDfactor, mean)
CDDmean <- tapply(CDD, CDDfactor, mean)

HDDmean <- as.vector(HDDmean)
CDDmean <- as.vector(CDDmean)

##plot
years <- seq(from = 1944, to = 2018, by = 1)
plot(years, CDDmean, cex = 0.5, col = 'red', type = 'l',
     ylim = c(0,5), xlim = c(1940, 2020), main = 'CDD')
lines(lowess(years, CDDmean))

plot(years, HDDmean, cex = 0.5, col = 'blue', type = 'l', xlim = c(1940, 2020),
     main = 'HDD')
lines(lowess(years, HDDmean))

```

###3b
    
    For missing value, determine the missing value day belongs to which season and use corresponding seanson average to replace the missing temperature.

###3c

```{r}
##histogram
hist(HDDmean,freq = F, col = 'blue')
lines(density(HDDmean))
hist(CDDmean,freq = F,col = 'red', ylim = c(0,1))
lines(density(CDDmean))

```


```{r}
boxplot(HDDmean, main = 'HDD')
boxplot(CDDmean, main = 'CDD')
```
    
    The existing outliers ruined the symmetry.
    

##3d
```{r}
CDDpre <- CDDmean[1:37] 
CDDpost <- CDDmean[38:75]
HDDpre <- HDDmean[1:37] 
HDDpost <- HDDmean[38:75]
t.test(CDDpre, CDDpost, alternative = "two.sided")
t.test(HDDpre, HDDpost, alternative = 'greater')
```
    
    Since an obvious descending trend in HDD can be observed, I chose a one-side t-test for HDD, the P-value is 0.0003124 < 0.005, which means under a 95% confidence level, significant difference exists among HDD before and after 1980.
    
    When apply a two-side t-test for CDD, we can not refuce the null hypothesis(average CDD before and after 1980 are same) at 95% confidence level.

##3e

```{r echo=TRUE}
years <- years - mean(years)
summary(lm(CDDmean~years))
plot(years,CDDmean)
lines(years,fitted(lm(CDDmean~years)))
```
    
    The estimator is positive which can indacte a ascending trend of CDD

```{r}
summary(lm(HDDmean~years))
plot(years,HDDmean)
lines(years,fitted(lm(HDDmean~years)))
```
    
    The slope is negative which can indacte a descending trend of HDDï¼Œwhich is consistent with the result of t-test did before.

##4a
```{r}
seasonfactor <- as.factor(c(1,1,2,2,2,3,3,3,4,4,4,1))
seasonmean <- function(x){
  tapply(x, seasonfactor, mean)
}

nino34 <- read.table('nino34a.txt')
nino34 <- as.matrix(nino34[, -1])
nino34SA <- apply(nino34, MARGIN = 1, FUN = seasonmean)
nino34SON <- nino34SA[4, (1:61)]
nino34MAM <- nino34SA[2, (1:61)]

nino12 <- read.table('nino12a.txt')
nino12 <- as.matrix(nino12[, -1])
nino12SA <- apply(nino12, MARGIN = 1, FUN = seasonmean)#SA=seasonal average
nino12SON <- nino12SA[4, (1:61)]
nino12MAM <- nino12SA[2, (1:61)]

PDO <- read.table('PDO.txt')
PDO <- PDO[, -1]
PDOSA <- apply(PDO, MARGIN = 1, FUN = seasonmean)
PDOSON <- PDOSA[4, (1:61)]
PDOMAM <- PDOSA[2, (1:61)]

GTO <- read.table('GTO.txt')
GTO <- GTO[, -1]
GTOSA <- apply(GTO, MARGIN = 1, FUN = seasonmean)
GTOSON <- GTOSA[4, (1:61)]
GTOMAM <- GTOSA[2, (1:61)]

solarirr <- read.table('solarirr.txt', header = F)
solarirr <- solarirr[, -1]
solarirrSA <- apply(solarirr, MARGIN = 1, FUN = seasonmean)
solarirrSON <- solarirrSA[4, (1:61)]
solarirrMAM <- solarirrSA[2, (1:61)]

ninoPI <- read.table('preInino34.txt') #precipitation indecis
ninoPI <- ninoPI[, -1]
ninoPISA <- apply(ninoPI, MARGIN = 1, FUN = seasonmean)
ninoPISON <- ninoPISA[4, (1:61)]
ninoPIMAM <- ninoPISA[2, (1:61)]

nino4 <- read.table('nino4ab.txt') #precipitation indecis
nino4 <- nino4[, -1]
nino4SA <- apply(nino4, MARGIN = 1, FUN = seasonmean)
nino4SON <- nino4SA[4, (1:61)]
nino4MAM <- nino4SA[2, (1:61)]

minSON <- SON_min[-(1:91)]
minSON <- minSON[1:(61*91)]
SONfactor <- as.factor(mapply(rep, (1:61), times = rep(91, 61)))
minSON <- as.vector(tapply(minSON, SONfactor, mean))
```


##4b

    For CDD, I chose nino3.4, pacific decadal oscillation, and global temperature on North hemisphere as potential predictors.
    For HDD, I chose nino12, solar irridiance, and nino precipitation index as potential predictors.

```{r}
XCDD <- as.matrix(data.frame(NINO34 = nino34MAM-mean(nino34MAM), 
                             PDO = PDOMAM-mean(PDOMAM), GTO = GTOMAM-mean(GTOMAM)))
XCDDT <- t(XCDD)
coCDD <- XCDDT %*% XCDD
coCDD <- coCDD / 60
coCDD 

XHDD <- as.matrix(data.frame(NINO12 = nino12SON-mean(nino12SON), 
                             SOLAR = solarirrSON-mean(solarirrSON), 
                             NINOPI = ninoPISON-mean(ninoPISON)))
XHDDT <- t(XHDD)
coHDD <- XHDDT %*% XHDD #collinear = singular?\
coHDD <- coHDD / 60
coHDD

```

    The covariance between the different variables are all much far from 1. However, multicollinearity may also exist, thus use VIF to make a more solid conclusion.

```{r message=FALSE, warning=FALSE}
library(car)
vif(lm(CDDmean[1:61]~nino34MAM + PDOMAM + GTOMAM))
vif(lm(HDDmean[1:61]~nino12SON + solarirrSON + ninoPISON))
```
    
    No VIF is bigger than 4, thus we don't need to worry about multicollinearity

##4c

### stepwise linear regression 
```{r}
library(MASS)
TIME <- 1:61
stepAIC(lm(CDDmean[1:61]~nino34MAM + PDOMAM + GTOMAM + TIME))

```

    The model for predicting CDD should be (p.s. GTO means global temperature on North hemisphere ocean)
$$
CDD_i=0.59702 + -0.50945*GTO_i + 0.01016*year_i
$$


```{r}
stepAIC(lm(HDDmean[1:61]~nino12SON + solarirrSON + ninoPISON + TIME))
```

    The model for HDD should be
$$
HDD_i=2.5734 + -0.0238*year_i
$$

### Uncertainty of parameters

```{r}
uncCDD <- (summary(lm(CDDmean[1:61]~TIME + GTOMAM))$coefficients[c(2,3),2])^2
uncHDD <- (summary(lm(HDDmean[1:61]~TIME))$coefficients[2,2])^2

cat('the variance of parameter of predictor time and global ocean temperature on North hemisphere is',uncCDD)
cat('\nthe variance of parameter of predictor time is',uncHDD)
```

##4d

    Select seasonal average minimum temperature of SON as new predictor for HDD, seasonal average maximum temperature of MAM as new predictor for CDD
    
```{r}

minSON <- SON_min[-(1:91)]
minSON <- minSON[1:(61*91)]
SONfactor <- as.factor(mapply(rep, (1:61), times = rep(91, 61)))
minSON <- as.vector(tapply(minSON, SONfactor, mean))

maxMAM <- MAM_max[1:(61*92)]
MAMfactor <- as.factor(mapply(rep, (1:61), times = rep(92, 61)))
maxMAM <- as.vector(tapply(maxMAM, MAMfactor, mean))

library(MASS)
stepAIC(lm(CDDmean[1:61]~nino34MAM + PDOMAM + GTOMAM + TIME + maxMAM))
```

    Model for CDD now should be
$$
CDD_i=-1.9672 + 0.1445*maxMAM_i
$$

```{r}
stepAIC(lm(HDDmean[1:61]~nino12SON + solarirrSON + ninoPISON + TIME + minSON))
```

    Model for HDD now should be

$$
HDD_i=5.55132 -0.01709*year_i-0.22217*minSON_i
$$
  
    Models for both HDD and CDD changed, the function of LA temperature are both included as a predictor. For CDD, the old pridectors time and global ocean temperature on North hemisphere are removed. 
   
##4e
```{r}
summary(lm(CDDmean[1:61]~maxMAM))
summary(lm(CDDmean[1:61]~GTOMAM + TIME))
cddlm <- lm(CDDmean[1:61]~maxMAM)
uncCDD <- (summary(cddlm)$coefficients[2,2])^2
b1CDDP <- 0.14452 + uncCDD
b1CDDM <- 0.14452 - uncCDD

maxMAMpre <- MAM_max[(61*92+1):length(MAM_max)]
MAMfactor <- as.factor(mapply(rep, (1:13), times = rep(92, 13)))
maxMAMpre <- as.vector(tapply(maxMAMpre, MAMfactor, mean))

preCDD <- -1.96721 + 0.14452 * maxMAMpre
#plot(preCDD,col = 'blue',type = 'l', ylim = c(0,2))
#lines(CDDmean[62:74], type = 'l')
CDDpre <- predict(cddlm, newdata = data.frame(maxMAM=maxMAMpre), interval = 'confidence',level = 0.95)
plot(CDDpre[,1], type = 'p', ylim = c(0, 3), xlab = 'time', ylab = 'CDD')
lines(CDDpre[,2], type = 'p',col = 'blue')
lines(CDDpre[,3], type = 'p',col = 'red')
lines(CDDmean[62:74], type = 'p')
legend('topright', title = 'Methods', c('upper uncertatinty','lower uncertainty'), 
       lty = c(1,1), col = c('red','blue'),cex = 0.6)

```
```{r}

summary(lm(HDDmean[1:61]~TIME))
summary(lm(HDDmean[1:61]~TIME + minSON))

hddlm <- lm(HDDmean[1:61]~TIME + minSON)
unctime <- (summary(hddlm)$coefficients[2,2])^2
uncSON <- (summary(hddlm)$coefficients[3,2])^2
minSONpre <- SON_min[-(1:91)]
minSONpre <- minSONpre[(61*91+1):length(minSONpre)]
SONfactor <- as.factor(mapply(rep, (1:13), times = rep(91, 13)))
minSONpre <- as.vector(tapply(minSONpre, SONfactor, mean))

HDDpre <- predict(hddlm, newdata = data.frame(TIME = 62:74, minSON = minSONpre), interval = 'confidence',level = 0.95)
plot(HDDpre[,1], type = 'l', ylim = c(0, 3), ylab = 'HDD', xlab = 
       'time')
lines(HDDpre[,2], type = 'l',col = 'blue')
lines(HDDpre[,3], type = 'l',col = 'red')
lines(HDDmean[62:74], type = 'p')
legend('topright', title = 'Methods', c('upper uncertatinty','lower uncertainty'), 
       lty = c(1,1), col = c('red','blue'),cex = 0.6)
```

##4g
    
    The residuals can be regard as independent, thus the block size is one.
    
```{r}
recdd <- residuals(cddlm)
rehdd <- residuals(hddlm)
ycdd <- fitted(cddlm)
yhdd <- fitted(hddlm)

maxMAMV <- c()
timeV <- c()
minSONV <- c()
ccdd <- c() #constant for cdd
chdd <- c()

##bootstrap
set.seed(567)
for (i in (1:1000)){
  recdd <- sample(recdd, 61)
  ycdd <- ycdd + recdd
  cddlmi <- summary(lm(ycdd ~ maxMAM))
  ccdd[i] <- cddlmi$coefficients[1,1]
  maxMAMV[i] <- cddlmi$coefficients[2,1]
  
  rehdd <- sample(rehdd, 61)
  yhdd <- yhdd + rehdd
  hddlmi <- summary(lm(yhdd ~ TIME + minSON))
  chdd[i] <- hddlmi$coefficients[1,1]
  timeV[i] <- hddlmi$coefficients[2,1]
  minSONV[i] <- hddlmi$coefficients[3,1]
  ycdd <- fitted(cddlm)
  yhdd <- fitted(hddlm)
 
}
Opar <- par(no.readonly = T) # store the original settings
par(mfrow=c(2,2),ann=F)
hist(timeV, freq = F,breaks = 'Scott', main = 'time')
lines(density(timeV,bw='nrd'))
hist(minSONV, freq = F,breaks = 'Scott', main = 'minSON')
lines(density(minSONV,bw='nrd'))
hist(maxMAMV, freq = F, ylim = c(0,10),breaks = 'Scott', main = 'maxMAM')
lines(density(maxMAMV,bw='nrd'))
hist(ccdd, freq = F, breaks = 'Scott', main = 'Intercept of CDD')
lines(density(ccdd,bw='nrd'))
hist(chdd, freq = F, breaks = 'Scott', main = 'Intercept of HDD')
lines(density(chdd,bw='nrd'))
#plot(density(residuals(hddlm)), main = 'residuals for HDD')
#plot(density(residuals(cddlm)), main = 'residuals for CDD')
par(Opar)

confint(hddlm)
confint(cddlm)
cat('\nvariability of maxMAM is ', min(maxMAMV),'to', max(maxMAMV),'\n')
cat('variability of minSON is ', min(minSONV),'to', max(minSONV),'\n')
cat('variability of time is ', min(timeV),'to', max(timeV),'\n')
```

    The distribution for those parameters are close to normal distribution.
    
```{r}
Opar <- par(no.readonly = T) # store the original settings
par(mfrow=c(2,2),ann=F)
plot(cddlm)
par(Opar)

Opar <- par(no.readonly = T) # store the original settings
par(mfrow=c(2,2),ann=F)
plot(hddlm)
par(Opar)
```

    
    Basically, the distributions of residuals for both HDD and CDD fit the mormal distribution, and the scale-location plots verified the Homogeneity of variance. A linear model is suitable in this case, but the reason why it results in a bad model is because the outliers in original data make things difficult, also the sample size is not enough.
